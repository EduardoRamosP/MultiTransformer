# Multi-Transformer: A new neural network-based architecture for forecasting S&P volatility
Transformer layers have already been sucessfully applied for NLP purposes. This repository adapts Transfomer layers in order to be used within hybrid volatility forecasting models. Following the intuition of bagging, this repository also introduces Multi-Transformer layers. The aim of this novel architecture is to improve the stability and accurateness of Transformer layers by averaging multiple attention mechasims.

The article collecting theoretical background and empirical results of the proposed model can be downloaded [*here*](https://doi.org/10.3390/math9151794).
