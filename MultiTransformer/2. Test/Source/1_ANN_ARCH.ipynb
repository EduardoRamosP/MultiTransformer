{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries needed for running the models.\n",
    "\n",
    "This program imports all the libraries needed for running the ANN-GARCH model. Please install all the libraries before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# python -m pip install yahoo-finance; pip install yahoo-finance; pip install yfinance --upgrade --no-cache-dir\n",
    "import yfinance as yf\n",
    "# pip install --upgrade ta; pip install ta\n",
    "import ta as ta\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import t\n",
    "import tensorflow as tf\n",
    "from datetime import date, datetime, timedelta\n",
    "from arch import arch_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for fitting ANN-GARCH are defined.\n",
    "\n",
    "This program is used for loading all the formulas needed for fitting ANN-GARCH model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Close'][i+dif])-np.log(Database['Close'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data.dropna()\n",
    "\n",
    "#Fitting of GARCH(1,1)\n",
    "def GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GARCH11 = arch_model(AR_Data, dist ='t')\n",
    "    res_GARCH11 = GARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_GARCH11 = res_GARCH11.conditional_volatility\n",
    "    For_CV_GARCH11 = np.array(res_GARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return GARCH11, res_GARCH11, CV_GARCH11, For_CV_GARCH11\n",
    "\n",
    "#Fitting of GJR_GARCH(1,1)\n",
    "def GJR_GARCH_Model_Student (Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    GJR_GARCH11 = arch_model(AR_Data, p=1, o=1, q=1, dist ='t')\n",
    "    res_GJR_GARCH11 = GJR_GARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_GJR_GARCH11 = res_GJR_GARCH11.conditional_volatility\n",
    "    For_CV_GJR_GARCH11 = np.array(res_GJR_GARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return GJR_GARCH11, res_GJR_GARCH11, CV_GJR_GARCH11, For_CV_GJR_GARCH11\n",
    "\n",
    "#Fitting of TARCH(1,1)\n",
    "def TARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    TARCH11 = arch_model(AR_Data, p=1, o=1, q=1, power=1.0, dist ='t')\n",
    "    res_TARCH11 = TARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_TARCH11 = res_TARCH11.conditional_volatility\n",
    "    For_CV_TARCH11 = np.array(res_TARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return TARCH11, res_TARCH11, CV_TARCH11, For_CV_TARCH11\n",
    "\n",
    "#Fitting of EGARCH(1,1)\n",
    "def EGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    EGARCH11 = arch_model(AR_Data, dist ='t', vol=\"EGARCH\")\n",
    "    res_EGARCH11 = EGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_EGARCH11 = res_EGARCH11.conditional_volatility\n",
    "    For_CV_EGARCH11 = np.array(res_EGARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return EGARCH11, res_EGARCH11,CV_EGARCH11, For_CV_EGARCH11\n",
    "\n",
    "#Fitting of Absolute Value GARCH(1,1)\n",
    "def AVGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    AVGARCH11 = arch_model(AR_Data, dist ='t', power=1)\n",
    "    res_AVGARCH11 = AVGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_AVGARCH11 = res_AVGARCH11.conditional_volatility\n",
    "    For_CV_AVGARCH11 = np.array(res_AVGARCH11.forecast(horizon=1).variance.dropna())[0][0]\n",
    "    return AVGARCH11, res_AVGARCH11, CV_AVGARCH11, For_CV_AVGARCH11\n",
    "\n",
    "#Fitting of FIGARCH11(1,1)\n",
    "def FIGARCH_Model_Student(Data):\n",
    "    AR_Data=Data['DailyReturns']*100\n",
    "    FIGARCH11 = arch_model(AR_Data, dist ='t', vol=\"FIGARCH\")\n",
    "    res_FIGARCH11 = FIGARCH11.fit(disp='off',options={'maxiter': 1000})\n",
    "    CV_FIGARCH11 = res_FIGARCH11.conditional_volatility\n",
    "    For_CV_FIGARCH11 = np.array(res_FIGARCH11.forecast(horizon=1).variance.dropna())[0][0]   \n",
    "    return FIGARCH11, res_FIGARCH11, CV_FIGARCH11, For_CV_FIGARCH11\n",
    "\n",
    "#AR models are fitted. As requested by arma package, returns are multiplied by 100 in order to improve the fitting process.\n",
    "#GARCH(1,1), GJR_GARCH(1,1), TARCH(1,1), EGARCH(1,1), AVGARCH(1,1) and FIGARCH(1,1) volatility models are fitted.\n",
    "#T student is assumed as distribution.\n",
    "def AR_Models (Data):\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    return GARCH_Parameters, CV_GARCH, For_CV_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH\n",
    "\n",
    "#Fully Connected model is created\n",
    "def FC_Model (Input_Dimension_NN, DropoutRate, LearningRate):\n",
    "    #Model struture is defined\n",
    "    Inputs = tf.keras.Input(shape=(Input_Dimension_NN,), name=\"Input\")\n",
    "    X = tf.keras.layers.Dense(16, activation=tf.nn.sigmoid, name=\"Dense1\") (Inputs)\n",
    "    X = tf.keras.layers.Dropout(DropoutRate) (X)\n",
    "    X = tf.keras.layers.Dense(8, activation=tf.nn.sigmoid, name=\"Dense2\") (X)\n",
    "    X = tf.keras.layers.Dropout(DropoutRate) (X)\n",
    "#    X = tf.keras.layers.Dense(4, activation=tf.nn.sigmoid, name=\"Dense3\") (X)\n",
    "#    X = tf.keras.layers.Dropout(DropoutRate) (X)\n",
    "    Output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, name=\"Output\") (X)\n",
    "    #Output = tf.keras.layers.Dropout(DropoutRate) (X)\n",
    "    model = tf.keras.Model(inputs=Inputs, outputs=Output)\n",
    "    #Optimizer is defined\n",
    "    Opt = tf.keras.optimizers.Adam(learning_rate=LearningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    #Model is compiled\n",
    "    model.compile(optimizer=Opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGenerationForecast (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index) \n",
    "    return Data\n",
    "\n",
    "#Final AR database for forcasting is generated\n",
    "def DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n",
    "    Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "    Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "    XDataForecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'], \n",
    "               'CV_GARCH' : For_CV_GARCH/100, 'CV_GJR_GARCH' : For_CV_GJR_GARCH/100, 'CV_TARCH' : For_CV_TARCH/100, \n",
    "               'CV_EGARCH' : For_CV_EGARCH/100, 'CV_AVGARCH' : For_CV_AVGARCH/100, 'CV_FIGARCH' : For_CV_FIGARCH/100}\n",
    "    return pd.DataFrame([XDataForecast], index=[Index_Forecast]), Data_Forecast['DailyReturns']\n",
    "\n",
    "#ANN-ARCH model forecast\n",
    "def ANN_ARCH_Forecast (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n",
    "    XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n",
    "    XDataForecast = pd.concat([XData_AR,XDataForecast])\n",
    "    XDataForecastTotalScaled_T = Scaled_Norm.transform(XDataForecast)\n",
    "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T)\n",
    "    return TransformerPrediction[-1][0], XDataForecast.index[-1], TransformerPrediction[0:(XDataForecastTotalScaled_T.shape[0]-1)], ReturnForecast\n",
    "\n",
    "#It calculates VaR taking into consideration the forecasted sigma to calculate the scale parameter\n",
    "def ANN_ARCH_VaR (Alpha, HistoricalReturns, ForecastedSigma, DF):\n",
    "    HistoricalMean = np.mean(HistoricalReturns)\n",
    "    ScaleParameter = np.sqrt((ForecastedSigma**2)*((DF-2)/DF))\n",
    "    VaR = -t.ppf(Alpha, DF, loc=HistoricalMean, scale=ScaleParameter)\n",
    "    return VaR\n",
    "\n",
    "#Formula to calculate the VaR of ARCH models\n",
    "def VaR_AR_Model (AR_Model,AR_Model_Results,Alpha):\n",
    "    Cond_Var=AR_Model_Results.forecast(horizon=1).variance.dropna()\n",
    "    Cond_Mean=AR_Model_Results.forecast(horizon=1).mean.dropna()\n",
    "    Quantile_Dist=AR_Model.distribution.ppf([Alpha], AR_Model_Results.params[-1:])\n",
    "    VaR=(-Cond_Mean-np.sqrt(Cond_Var)*Quantile_Dist)/100\n",
    "    return VaR.values\n",
    "\n",
    "#Formula to calculate the VaR of all the ARCH models\n",
    "def VaR_AR_Total(Alpha, GARCH_fit, GJR_GARCH_fit, TARCH_fit, EGARCH_fit, AVGARCH_fit, FIGARCH_fit,GARCH, GJR_GARCH, TARCH, EGARCH, AVGARCH, FIGARCH):\n",
    "    VaR_GARCH = VaR_AR_Model (GARCH,GARCH_fit,Alpha)\n",
    "    VaR_GJR_GARCH = VaR_AR_Model (GJR_GARCH,GJR_GARCH_fit,Alpha)\n",
    "    VaR_TARCH = VaR_AR_Model (TARCH,TARCH_fit,Alpha)\n",
    "    VaR_EGARCH = VaR_AR_Model (EGARCH,EGARCH_fit,Alpha)\n",
    "    VaR_AVGARCH = VaR_AR_Model (AVGARCH,AVGARCH_fit,Alpha)\n",
    "    VaR_FIGARCH = VaR_AR_Model (FIGARCH,FIGARCH_fit,Alpha)\n",
    "    return {'VaR_GARCH':VaR_GARCH, 'VaR_GJR_GARCH':VaR_GJR_GARCH, 'VaR_TARCH':VaR_TARCH, 'VaR_EGARCH':VaR_EGARCH, 'VaR_AVGARCH':VaR_AVGARCH, 'VaR_FIGARCH':VaR_FIGARCH}\n",
    "\n",
    "#Fitting of ANN-ARCH model and forecasting of the next volatility value\n",
    "def ANN_ARCH_Fit (Data, Lag=1, LagSD=5, Dropout=0.05, LearningRate=0.001, Epochs=10000, Alpha=0.005, DF=4, BatchSize=64):\n",
    "    #AR Models are fitted\n",
    "    GARCH, GARCH_Parameters, CV_GARCH, For_CV_GARCH = GARCH_Model_Student(Data)\n",
    "    GJR_GARCH, GJR_GARCH_Parameters, CV_GJR_GARCH, For_CV_GJR_GARCH = GJR_GARCH_Model_Student(Data)\n",
    "    TARCH, TARCH_Parameters, CV_TARCH, For_CV_TARCH = TARCH_Model_Student(Data)\n",
    "    EGARCH, EGARCH_Parameters,CV_EGARCH, For_CV_EGARCH = EGARCH_Model_Student(Data)\n",
    "    AVGARCH, AVGARCH_Parameters,CV_AVGARCH, For_CV_AVGARCH = AVGARCH_Model_Student(Data)\n",
    "    FIGARCH, FIGARCH_Parameters,CV_FIGARCH, For_CV_FIGARCH  = FIGARCH_Model_Student(Data)\n",
    "    #Database contaning AR models is generated\n",
    "    Data_AR=pd.concat([Data, CV_GARCH.rename('CV_GARCH')/100, CV_GJR_GARCH.rename('CV_GJR_GARCH')/100, CV_TARCH.rename('CV_TARCH')/100, \n",
    "                     CV_EGARCH.rename('CV_EGARCH')/100, CV_AVGARCH.rename('CV_AVGARCH')/100, CV_FIGARCH.rename('CV_FIGARCH')/100], axis=1)\n",
    "    if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "    #Original explanatory and response variables are generated\n",
    "    XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR_Norm_T = Data_AR['TrueSD']\n",
    "    #Data is normalized\n",
    "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm_T = Scaled_Norm.transform(XData_AR)\n",
    "    #Model with FC layers is defined\n",
    "    model = FC_Model(XData_AR_Norm_T.shape[1], DropoutRate=Dropout, LearningRate=LearningRate)\n",
    "    model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, verbose=0, batch_size=BatchSize); tf.keras.backend.clear_session()\n",
    "    Forecast, Date_Forecast, TrainPrediction, ReturnForecast = ANN_ARCH_Forecast (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n",
    "    VaR = ANN_ARCH_VaR(Alpha, Data['DailyReturnsOld'], Forecast,DF)\n",
    "    return {'ANN_ARCH_model':model, 'Forecast_ANN_ARCH':Forecast, 'Date_Forecast':Date_Forecast, 'TrainPrediction': TrainPrediction, 'Scaler':Scaled_Norm, 'Forecast_GARCH':For_CV_GARCH, 'Forecast_GJR_GARCH':For_CV_GJR_GARCH, 'Forecast_TARCH':For_CV_TARCH, 'Forecast_EGARCH':For_CV_EGARCH, 'Forecast_AVGARCH':For_CV_AVGARCH, 'Forecast_FIGARCH':For_CV_FIGARCH, 'ReturnForecast':ReturnForecast, 'GARCH_fit': GARCH_Parameters, 'GJR_GARCH_fit':GJR_GARCH_Parameters, 'TARCH_fit':TARCH_Parameters, 'EGARCH_fit':EGARCH_Parameters, 'AVGARCH_fit':AVGARCH_Parameters, 'FIGARCH_fit':FIGARCH_Parameters, 'GARCH': GARCH, 'GJR_GARCH':GJR_GARCH, 'TARCH':TARCH, 'EGARCH':EGARCH, 'AVGARCH':AVGARCH, 'FIGARCH':FIGARCH, 'YData_Train':YData_AR_Norm_T, 'VaR': VaR}\n",
    "#     return (model, Forecast, Date_Forecast, TrainPrediction, Scaled_Norm, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH, ReturnForecast, GARCH_Parameters, GJR_GARCH_Parameters, TARCH_Parameters, EGARCH_Parameters, AVGARCH_Parameters, FIGARCH_Parameters, GARCH, GJR_GARCH, TARCH, EGARCH, AVGARCH, FIGARCH, YData_AR_Norm_T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
